<div id="top" align="center">

<p align="center">
  <img src="images/title.png">
</p>


<!-- Download dataset [**HERE**](docs/data_prep_nus.md) (serves as Official source for `Autonomous Driving Challenge 2024`) -->
`2nd ITSS Student Competition` **Pedestrian Behavior Prediction** is [activated](https://github.com/OpenDriveLab/DriveLM/tree/main/challenge)!
</div>

<div id="top" align="center">

[![](https://img.shields.io/badge/Project%20Page-8A2BE2)](https://opendrivelab.com/DriveLM/)
[![License: Apache2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](#licenseandcitation)
[![arXiv](https://img.shields.io/badge/arXiv-2312.14150-b31b1b.svg)](https://arxiv.org/abs/2312.14150)
[![](https://img.shields.io/badge/Latest%20release-v1.1-yellow)](#gettingstarted)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DriveLM-ffc107?color=ffc107&logoColor=white)](https://huggingface.co/spaces/AGC2024/driving-with-language-2024)

<!-- <a href="https://opendrivelab.github.io/DriveLM" target="_blank">
    <img alt="Github Page" src="https://img.shields.io/badge/Project%20Page-white?logo=GitHub&color=green" />
  </a> -->

<!-- [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DriveLM-ffc107?color=ffc107&logoColor=white)](https://huggingface.co/datasets/OpenDrive/DriveLM) -->

</div>


## 2nd ITSS Student Competition in Pedestrian Behavior Prediction

:bulb: Welcome to the official repository for the 2nd ITSS Student Competition in Pedestrian Behavior Prediction! 

This competition is organized by the IEEE ITSS Technical Committee on **H**uman-Centered **AI** in **T**ransportation **(HAIT)** and focuses on advancing AI technologies for predicting pedestrian behavior in the context of intelligent transportation systems.

## Table of Contents
1. [Competition Overview](#overview)
2. [Current Endeavors and Future Horizons](#timeline)
3. [How to Participate](#participate)
   - [Not decided yet](docs/data_prep_nus.md)
4. [Dataset Information](#dataset)
5. [Evaluation Criteria](#criteria)
6. [Organizer](#organizer)
7. [License and Citation](#licenseandcitation)
8. [Other Resources](#otherresources)


## Competition Overview <a name="overview"></a>

ðŸ“š:**Background:** Predicting pedestrian behavior poses not only a critical challenge for fully automated cars when interacting with vulnerable road users but also holds broader implications in smart cities and intelligent infrastructures, aiming to enhance traffic safety and mobility efficiency. With the rapid progress of AI technologies and computing capabilities, much research has focused on developing diverse algorithms for predicting pedestrian behaviors. However, the absence of a standard evaluation framework and testing dataset makes it challenging to compare and assess different algorithms, hindering the seamless integration of AI-based behavior prediction models into practical systems. The overall performance of algorithms in behavior prediction is also far from satisfactory. Hence, it is imperative to encourage more development and testing activities in this significant area.

ðŸ“Œ**Goal:** Develop and test advanced AI algorithms capable of predicting pedestrian behaviors up to several seconds ahead, utilizing 3D trajectories and noisy sensor data, to enhance traffic safety and mobility efficiency in smart cities.

**Short-term track:** predict pedestrian behaviors up to 3 seconds ahead.

**Long-term track:** predict pedestrian behaviors up to 6 seconds ahead.

**(Attach a Video demonstrations, or poster, images?)**

<p align="right">(<a href="#top">back to top</a>)</p>


## Current Endeavors and Future Directions  <a name="timeline"></a>
> 

<p align="center">
  <img src="assets/images/repo/drivelm_timeline_v3.jpg">
</p>

(Briefly introduce current endeavors (maybe as timeline) ). Our contest attempts to address some of the challenges faced by the community.

- **Lack of standard metircs**:
- **challenges 2**
- **etc**

<p align="right">(<a href="#top">back to top</a>)</p>


## How to Participate <a name="participate"></a>

1. **Register** on the competition platform, Codalab.
2. **Download the Dataset** from the provided links after registration.
3. **Submit Your Models** on the Codalab competition page by the deadline.

<p align="right">(<a href="#top">back to top</a>)</p>


## Current Endeavors and Future Directions  <a name="timeline"></a>
> 

<p align="center">
  <img src="assets/images/repo/drivelm_timeline_v3.jpg">
</p>

Our contest attempts to address some of the challenges faced by the community.

- **Lack of standard metircs**:
- **challenges 2**
- **etc**

<p align="right">(<a href="#top">back to top</a>)</p>


## Dataset Information <a name="dataset"></a>

The competition dataset includes:
- Scene images
- LiDAR data
- Car localization raw inputs
Collected across five metropolitan areas in the USA, with additional datasets incorporating diverse social norms from various global locations.

<p align="right">(<a href="#top">back to top</a>)</p>

## Evaluation Criteria <a name="criteria"></a>

The models will be evaluated based on:
1. Accuracy of the 3D trajectory predictions.
2. Robustness to noisy inputs.
3. Efficiency of the computational algorithms.

<p align="right">(<a href="#top">back to top</a>)</p>

## Organizer <a name="organizer"></a>

<p align="right">(<a href="#top">back to top</a>)</p>

## License and Citation <a name="licenseandcitation"></a>
All assets and code in this repository are under the [Apache 2.0 license](./LICENSE) unless specified otherwise. The language data is under [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Other datasets (including nuScenes) inherit their own distribution licenses.

<p align="right">(<a href="#top">back to top</a>)</p>


## Other Resources <a name="otherresources"></a>
<a href="https://twitter.com/OpenDriveLab" target="_blank">
    <img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/OpenDriveLab?style=social&color=brightgreen&logo=twitter" />
  </a>

<!-- <a href="https://opendrivelab.com" target="_blank">
  <img src="https://img.shields.io/badge/contact%40opendrivelab.com-white?style=social&logo=gmail">
</a> -->

<!--
 [![Page Views Count](https://badges.toozhao.com/badges/01H9CR01K73G1S0AKDMF1ABC73/blue.svg)](https://badges.toozhao.com/stats/01H9CR01K73G1S0AKDMF1ABC73 "Get your own page views count badge on badges.toozhao.com")
-->

**OpenDriveLab**
- [DriveAGI](https://github.com/OpenDriveLab/DriveAGI) | [UniAD](https://github.com/OpenDriveLab/UniAD) | [OpenLane-V2](https://github.com/OpenDriveLab/OpenLane-V2) | [Survey on E2EAD](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving)
- [Survey on BEV Perception](https://github.com/OpenDriveLab/BEVPerception-Survey-Recipe) | [BEVFormer](https://github.com/fundamentalvision/BEVFormer) | [OccNet](https://github.com/OpenDriveLab/OccNet)

<a href="https://twitter.com/AutoVisionGroup" target="_blank">
    <img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/Awesome Vision Group?style=social&color=brightgreen&logo=twitter" />
  </a>

**Autonomous Vision Group**
- [tuPlan garage](https://github.com/autonomousvision/tuplan_garage) | [CARLA garage](https://github.com/autonomousvision/carla_garage) | [Survey on E2EAD](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving)
- [PlanT](https://github.com/autonomousvision/plant) | [KING](https://github.com/autonomousvision/king) | [TransFuser](https://github.com/autonomousvision/transfuser) | [NEAT](https://github.com/autonomousvision/neat)

<p align="right">(<a href="#top">back to top</a>)</p>
